Principal Component Regression (PCR)

 Now that you’ve learned how to fit factor models, what are they good for? In some settings, as in the previous political science example, the factors themselves have clear meaning and can be useful in their own right for understanding complex systems. More commonly, unfortunately, the factors are of dubious origin or interpretation. However, they can still be useful as inputs to a regression system. Indeed, this is the primary practical function for PCA—as the first stage of principal components regression (PCR). The concept of PCR is simple: instead of regressing y onto x, use a lower-dimension set of principal components as covariates. This is a fruitful strategy for a few reasons: •   PCA reduces dimension, which is usually good.

The PCs are independent, so you have no multicollinearity and the final regression is easy to fit. •   You might have far more unlabeled xi than labeled [xi, yi] pairs. This last point is especially powerful. You can use unsupervised learning (PCA) on a massive bank of unlabeled data and use the results to reduce dimension and facilitate supervised learning on a smaller set of labeled observations. The disadvantage of PCR is that PCA will be driven by the dominant sources of variation in x. If the response is connected to these dominant sources of variation, PCR works well. If it is more of a “needle in the haystack response,” driven by a small number of inputs, then PCR will not work well. For example, in finance it is commonly thought that equity returns are driven by a small number


of factors (see our CAPM discussion in the Introduction). If you want to be able to trade on what the rest of the market doesn’t know, then you will be looking for signal that is not summarized by these dominant factors. In such cases, PCR will not work well. In practice, you do not know what scenario you are in until you try both PCR and, say, a lasso regression on the raw x inputs.9 The two-stage PCR algorithm is straightforward: you run PCA and then run a regression procedure.

Codigo ejemplo PCR


Classically, the number of PCs to include in the regression can be selected using a version of subset selection where you build, say, p different models using PCs 1 through K for K = 1 . The optimal choice for K is then selected based on an information

information criterion or out-of-sample experimentation. Because the PCs are ordered (by their variance) and independent, this works better than subset selection on the raw dimensions of xi (which we’ve previously warned against). This PC selection procedure is fine, but in my experience it is both easier and better to simply run a lasso regression on the full set of PCs. You can then use the usual selection procedures to choose the λ regularization weight. This procedure makes it easy to incorporate other information in addition to the PCs. For example, one tactic that works well in practice is to put both v and x—both the PCs and the raw inputs—into the lasso model matrix. This then allows the regression to make use of the underlying factor structure in x and still pick up individual xij signals that are related to y. This hybrid strategy is a solution to the disadvantage

disadvantage of PCR mentioned earlier—that it will only pick up dominant sources of variation in x. ALGORITHM 19   Principal Components (Lasso) Regression Given a sample of regression input observations {xi}ni=1, accompanied by output labels yi for some subset of these observations: 1.

Fit PCA on the full set of xi inputs to obtain vi of length min (n, p). 2. For the labeled subset, run a lasso regression for yi on vi (including selection for penalty λ via either CV or AICc).

(a)   Alternatively, regress yi on xi and vi to allow simultaneous selection between PCs and raw inputs. To predict for a new xf, use the rotations from step 1 to get vf = Φxf and then feed these scores into the regression fit from step 2. In Algorithm 19, we are breaking one of the rules of model selection: we are manipulating the data outside of the CV loop when we fit PCA in step 1 on the full sample. However, it is fine to use the full x sample if you do not use the labels y outside of the OOS loop. So long as the random errors in your test y sample are not allowed to influence the model fit, then the result of your trained model on left-out data remains a good estimate of OOS performance. Indeed, the primary rule of CV is that you want an OOS experiment that mimics

how the models will be fit and used on true future data. It is potentially the case that you will know the x values for future observations and can include them in the PCA fitting even if you don’t yet know their labels. TO ILLUSTRATE PCR, LET’S CONSIDER TELEVISION DATA that include survey responses for focus groups on TV show pilots (first episodes of a new series) as well as the first year of ratings results (how many people ended up watching the show). The hope is that we can build a rule for predicting viewer interest from pilot surveys, thus helping the studios to make better programming decisions. The survey data include 6241 views and 20 questions for 40 shows. There are two types of questions in the survey. Both ask you the degree to which you agree with a statement. For Q1, this

statement takes the form of “This show makes me feelfeel . . . .” For Q2, the statement is “I find this showfeel . . . .” We have a couple of interesting outcome variables. Classic measures of broadcast marketability are ratings. Specifically, gross ratings points (GRP) provide an estimated count of total viewership. In this data we also track the projected engagement (PE) as a more subtle measure of audience attention. After watching a show, viewers are contacted and quizzed on order and detail for events in the show. This measures their engagement with the show (and, perhaps more importantly, the ads shown). PE is reported on a 0 to 100 scale, with 100 being fully engaged and 0 meaning they didn’t pay attention at all. Engagement matters on its own, as a driver of TRP and GRP, and also as an adjustment factor—for example, normalizing GRP/PE to get adjusted GRP.

We have these GRP and PE results for all 40 shows over their first year. The comparison is shown in Figure 7.15, with the programs differentiated by their genre—reality, comedy, or drama/adventure scripted series. Notice that higher engagement does tend to correspond to higher ratings but that comedies can have high engagement with lower ratings (i.e., they look better by adjusted GRP—GRP/PE—than by raw GRP). Reality shows tend to have lower engagements and lower ratings (but they are cheap to produce).

It might seem like there is a lot of data here—6241 pilot viewings—but there are only 40 shows and 20 survey questions. That is, there are only two observed y values for each input dimension, such that it is a small dataset with big data dimensionality

dimensionality problems. To relate survey results to show performance, we need to first calculate the average survey question response by show. This leads to a 40 × 20 design matrix X, and we can fit PCA on this design.

Looking at the PC rotation output shown, PC1 seems to have simple interpretation as the “how much you dislike the show” factor (i.e., negative PC1 is a likability factor). A show scores low on PC1 if it made viewers feel excited and engaged and if the material was original, entertaining, and suspenseful. A show scores high on PC1 if people found it annoying and boring. PC2 is less clearly interpretable: you score high on PC2 if you find the show boring, confusing, and predictable, but also if you find it funny. Figure 7.16 provides some insight. We see that the reality TV shows score high on both PC1 and PC2—they are both unlikable and can be annoying while funny—and the scripted dramas score low on both.

Partial Least Squares In the previous two examples, there was a clear low-dimensional factor structure in x: ideology in Congress and like-versus-dislike in the TV pilot survey. For the TV pilots, these factors were also directly related to the y response of interest. Nature will not always be this nice. It is common to encounter x data that has been generated without a clear factor structure, or through some messy mix of underlying factors and idiosyncratic shocks. And even when there is factor structure in x, it will often be that y is not related to the dominant sources of variation in x. The response is not driven by the first few PCs, and it is inefficient to try to estimate v as a middle-man between y and x. PCR will work only if the dominant directions of variation in x are related to y. However, the idea of combining inputs into a few factors (or indices) that affect y is an appealing framework. This strategy can lead to easier interpretation than a high-dimensional lasso onto raw inputs. Is there a way to force factors v to be relevant to both x and y? The answer is yes; this is referred to as supervised

supervised factor modeling, and it is a useful big data technique. There is a big world of supervised factor modeling, and there are several algorithms for supervised adaptations of PCA.11 We’ll consider the simple but powerful method of partial least squares (PLS), a supervised factorization strategy that has its roots in 1970s chemometrics12 but has been re-invented many times since. To understand PLS, we start with the more basic algorithm of marginal regression (MR). In this scheme, you simply regress y on to each dimension of x independently and then use the resulting regression coefficients to map from x to a univariate factor v. This factor aggregates the first-order effect of each input variable on y. It will be dominated by xj dimensions that both (1) have a big effect on y and (2) move consistently in the same

direction with each other (since their influence on the factor is additive). That is, marginal regression constructs a single factor that is connected both to y and to a dominant direction of variation in x. It yields a supervised factor. ALGORITHM 20   Marginal Regression To build a model for prediction of y from x: •   Calculate φ = [φ1 . . . φp] where φj = cor (xj, y)/sd(xj) is the OLS coefficient in a simple univariate regression for y on xj. •   Set vi = x′iφ = ∑j xij φj for each observation i. •   Fit the “forward” univariate linear regression yi = α + βνi + εi.

Given new xf, you can then predict . One big advantage of MR is computational efficiency. The rise of distributed computing has led to a rediscovery of MR because it is easy to code in the MapReduce framework. In the Map step, you produce [xij, yi] pairs that are indexed by the dimension key j; the Reduce step then runs univariate OLS for y on xj and returns φj. A second quick MapReduce algorithm is used to calculate vi = x′iφ and run the forward regression for y on v. Another advantage of MR (over, say, OLS) is that it works in arbitrarily high dimensions, even if p >> n. MR is a strategy for supervised learning in ultra-high dimensions. As an example, let’s look at the problem of mapping from chemical properties of gasoline to its octane rating—a key measure of quality that

determines pricing at the pump. In traditional practice, this octane is measured by running the fuel in a test engine under different compression magnitudes. The compression ratio where the fuel ignites determines its octane. Lower-cost octane testing could be possible through use of near-infrared (NIR) spectroscopy, which measures reflectance for light at wavelengths (around 400 here) that are longer than visible light. The path of the NIR measurements across wavelengths provides a signature of the underlying chemical properties. The goal in this example is to build a regression map between the NIR values x and the octane y. We have 60 gas samples and 401 wavelengths, so this is a case where p >> n. Figure 7.19 shows the x NIR reflectance measurements over the range of wavelengths. There is a clear structure in the NIR x—the measurements move as a smooth function

PARTIAL LEAST SQUARES IS AN EXTENSION OF MARGINAL REGRESSION. Instead of stopping after running the single MR, you iterate: take the residuals from the first MR and repeat a second MR to predict these residuals. You can then take the

residuals from the second MR and repeat, continuing until you reach the minimum of p and n. ALGORITHM 21   Partial Least Squares (PLS) Begin by running MR from Algorithm 20 for y on x. Store the MR factor as v1, the 1st PLS direction, and PLS(1) forward regression fitted values as ŷ1 = α + β1 · v1. Then, for k = 2 . . . K, calculate the following:    •   Residuals     •   Loadings     •   Fitted values 

This yields PLS rotations Φ = [φ1 . . . φK] and factors V = [v1 . . . vK]. The PLS routine in Algorithm 21 involves a number of steps but is really very simple. We are just running marginal regression on the residuals after each PLS(k) fit and updating the fitted values. This general procedure of taking a simple algorithm and repeatedly applying it to residuals from previous fits is called boosting. That is, PLS is boosted marginal regression. Boosting13 is a general and powerful machine learning technique. Although we don’t cover it in detail in this text (we focus on the related technique of bagging in Chapter 9), it is a useful way to add flexibility to simple methods, and you will likely encounter it often in your data science travels.

If p < n and you do PLS with K = p, then the fitted ŷKi will be the same as what you would get running OLS for y on x. The PLS coefficients on each xij, available as ∑k βkφkj, also match the OLS coefficients. Thus, PLS provides a path of models between MR and OLS. Whenever you are boosting, there is a potential for overfit. You should use OOS experimentation to select where the boosting should stop—i.e., to select K for the PLS(K) prediction model. Unlike with PCR, the y values are used in construction of the vk factors; hence, you can’t simply dump everything into gamlr and get a valid OOS experiment. In addition, there is no easy way to know the degrees of freedom for each PLS(K) fit, so you can’t apply tools like the AICc. You need to run an OOS experiment where PLS is run on a data subset

and you evaluate predictive performance on a left-out sample. The textir package in R has a pls function for running partial least squares, along with the usual utilities of summary, plot, and so on. We can use this to fit PLS for K ≤ 3 on the gas data:
using the pls function:



